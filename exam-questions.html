<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<!--
  Dmytro Tarasiuk
  taskcalc@membrama.com
*/

-->
<html xmlns="http://www.w3.org/1999/xhtml" lang="uk">
<head>
  <meta charset="utf-8">
  <link rel="stylesheet" type="text/css" href="styles.css">
  <!--Roboto Font-->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Pacifico&family=Roboto+Condensed:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
  <!--/-->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>  
  <script src="https://cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js "></script>
</head>
<!----------------------------------------------------------------------------------------------->
<body bgcolor="lightgray">
<center>
<table id="questions-table" class="questions-table">
  <tr><th colspan="2"><h1>Екзаменаційні питання</h1></th></tr>
  <tr><th>1</th><td>Етапи процесу вивчення даних (етапи в <code>Data</code> <code>Science</code>)</td></tr>
  <tr><th>2</th><td>Машинне навчання з вчителем</td></tr>
  <tr><th>3</th><td>Машинне навчання без вчителя. Основні задачі</td></tr>
  <tr><th>4</th><td>Базові типи задач, які вирішують системи штучного інтелекту. Різниця між класифікацією та регресією</td></tr>
  <tr><th>5</th><td>Навчання моделі системи штучного інтелекту. Недонавчання. Перенавчання</td></tr>
  <tr><th>6</th><td>Боротьба з перенавчанням. Регуляризатори <code>L<sub>1</sub></code>, <code>L<sub>2</sub></code> та їх властивості</td></tr>
  <tr><th>7</th><td>Метод опорних векторів. Ядра <code>SVM</code></td></tr>
  <tr><th>8</th><td>Метод опорних векторів. Зв’язок зі штучними нейронними мережами</td></tr>
  <tr><th>9</th><td>Дерева рішень. Ліс рішень</td></tr>
  <tr><th>10</th><td>Ансамблювання слабких моделей. <code>Bagging</code>, <code>Random</code> <code>Forest</code>, <code>Boosting</code></td></tr>
  <tr><th>11</th><td>Лінійна регресія. Функція втрат. Метод градієнтного спуску</td></tr>
  <tr><th>12</th><td>Лінійна регресія. Вибір початкових значень параметрів, що мінімізують функцію втрат</td></tr>
  <tr><th>13</th><td>Нелінійна регресія. Функція втрат. Локальні мінімуми</td></tr>
  <tr><th>14</th><td>Вектори, матриці і тензори. Коваріація, кореляція. Косинусна подібність</td></tr>
  <tr><th>15</th><td>Кластеризація. Метод «ліктя»</td></tr>
  <tr><th>16</th><td>Штучні нейронні мережі як множина регресій</td></tr>
  <tr><th>17</th><td>Математична модель штучного нейрона</td></tr>
  <tr><th>18</th><td>Аналогова реалізація моделі штучного нейрона</td></tr>
  <tr><th>19</th><td>Задачі штучних нейронних мереж</td></tr>
  <tr><th>20</th><td>Функція активації. Властивості. Різновиди</td></tr>
  <tr><th>21</th><td>Теорема Колмогорова А.М. про універсальність штучних нейронних мереж</td></tr>
  <tr><th>22</th><td>Проєктування архітектури штучної нейронної мережі для вирішення практичної задачі</td></tr>
  <tr><th>23</th><td>Види штучних нейронних мереж</td></tr>
  <tr><th>24</th><td>Навчання штучної нейронної мережі. Пряме розповсюдження сигналу. <code>Backpropagation</code></td></tr>
  <tr><th>25</th><td>Методи боротьби з недонавчанням штучних нейронних мереж</td></tr>
  <tr><th>26</th><td>Методи боротьби з перенавчанням штучних нейронних мереж. Регуляризація <code>Dropout</code></td></tr>
  <tr><th>27</th><td>Методи боротьби з перенавчанням штучних нейронних мереж. Рання зупинка</td></tr>
  <tr><th>28</th><td>Навчання штучних нейронних мереж. Оптимізатори. Градієнтний спуск з моментом</td></tr>
  <tr><th>29</th><td>Навчання штучних нейронних мереж. Оптимізатори. Оптимізатор <code>Adam</code></td></tr>
  <tr><th>30</th><td>Проблеми навчання штучних нейронних мереж: зникаючий градієнт</td></tr>
  <tr><th>31</th><td>Проблеми навчання штучних нейронних мереж: вибухаючий градієнт</td></tr>
  <tr><th>32</th><td>Проблеми навчання штучних нейронних мереж: незбалансовані дані. Зважування класів для калібрування результату навчання</td></tr>
  <tr><th>33</th><td>Проблеми навчання штучних нейронних мереж: недостатня кількість даних. Аугментація</td></tr>
  <tr><th>34</th><td>Чому так добре працюють нейронні мережі? Гіпотеза про многовид (<code>Manifold</code> <code>Hypothesis</code>)</td></tr>
  <tr><th>35</th><td>Ембедінг слів. N-грами. <code>Word2Vec</code>. Взаємозв’язок між сенсами слів та косинусною подібністю їх векторів</td></tr>
  <tr><th>36</th><td>Глибокі повнозв’язні штучні нейронні мережі</td></tr>
  <tr><th>37</th><td>Згорткові штучні нейронні мережі. Відмінність згорткових нейронних мереж від повнозв’язних</td></tr>
  <tr><th>38</th><td>Згорткові штучні нейронні мережі. Ядра згортки. Фільтри</td></tr>
  <tr><th>39</th><td>Згорткові штучні нейронні мережі. Ефекти меж та доповнення. Крок згортки</td></tr>
  <tr><th>40</th><td>Згорткові штучні нейронні мережі. <code>MaxPooling</code>. <code>AveragePooling</code></td></tr>
  <tr><th>41</th><td>Згорткові штучні нейронні мережі. <code>Computer</code> <code>Vision</code>. Основні задачі та приклади використання</td></tr>
  <tr><th>42</th><td>Навчання різниці між вхідним та вихідним сигналом. З’єднання з пропуском. Залишкові штучні нейронні мережі</td></tr>
  <tr><th>43</th><td>Залишкові штучні нейронні мережі як спосіб боротьби з деградацією точності</td></tr>
  <tr><th>44</th><td>Залишкові штучні нейронні мережі. Переваги та недоліки</td></tr>
  <tr><th>45</th><td>Моделювання та аналіз послідовностей. Рекурентні штучні нейронні мережі</td></tr>
  <tr><th>46</th><td>Навчання рекурентних штучних нейронних мереж. Усічене зворотне поширення (<code>truncated</code> <code>backpropagation</code>)</td></tr>
  <tr><th>47</th><td>Обмеження класичних рекурентних штучних нейронних мереж</td></tr>
  <tr><th>48</th><td>Рекурентні штучні нейронні мережі. Процес запам’ятовування і забування. Вентильно рекурентна одиниця обчислення (<code>GRU</code>)</td></tr>
  <tr><th>49</th><td>Рекурентні штучні нейронні мережі. Довго-коротко термінова пам’ять (<code>LSTM</code>)</td></tr>
  <tr><th>50</th><td>Рекурентні штучні нейронні мережі. Переваги та недоліки</td></tr>
  <tr><th>51</th><td><code>LSTM</code> чат-боти. <code>Encoder</code>. <code>Decoder</code></td></tr>
  <tr><th>52</th><td>Двонаправлені рекурентні штучні нейронні мережі</td></tr>
  <tr><th>53</th><td>Трансформери. Механізм самоуваги</td></tr>
  <tr><th>54</th><td><s>Трансформери. Матриці <code>Query</code>, <code>Key</code>, <code>Value</code>. Аналогії для розуміння</s></td></tr>
  <tr><th>55</th><td>Трансформери. Переваги та недоліки</td></tr>
  <tr><th>56</th><td>Кластеризація. Основна мета. Види</td></tr>
  <tr><th>57</th><td>Кластеризація <code>KMeans</code> (K-середніх)</td></tr>
  <tr><th>58</th><td>Кластеризація. Як інтерпретувати кластери</td></tr>
  <tr><th>59</th><td><s>Кластеризація. Переваги та недоліки</s></td></tr>
  <tr><th>60</th><td>Зменшення розмірності. Метод головних компонент (<code>PCA</code>)</td></tr>
  <tr><th>61</th><td>Контентно-орієнтовані рекомендаційні системи</td></tr>
  <tr><th>62</th><td>Колаборативні рекомендаційні системи</td></tr>
</table>
</center>
</body>

<script>
  $(function() {
    console.log("Loaded...");
    const fullname = Cookies.get('fullname') || '';
    const widn = Cookies.get('widn') || '';
    if (widn.length == 0 || fullname.length == 0) {
      $('body').html('<center>'
        + '<h1>Ви не слухач курсу лекцій "Системи штучного інтелекту" і Вам здавати екзамен не потрібно!</h1>'
        + '</center>')
    }
  });
</script>
</html>
